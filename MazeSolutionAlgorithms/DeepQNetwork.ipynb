{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from MazeGenerationAlgorithms.RandomizedKruskal import RandomizedKruskalMaze\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import scipy.special as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from IPython.display import clear_output\n",
    "from collections import namedtuple, deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"files\"\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    os.remove(file_path)  \n",
    "\n",
    "print(f'All contents of \"{folder_path}\" have been deleted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Memory',\n",
    "                        field_names=['state', 'action', 'next_state', 'reward', 'is_game_on'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size, device='mps'):\n",
    "        transitions = random.sample(self.memory, batch_size)\n",
    "        states, actions, next_states, rewards, isgameon = zip(*transitions)\n",
    "        return (torch.tensor(states, dtype=torch.float, device=device),\n",
    "                torch.tensor(actions, dtype=torch.long, device=device),\n",
    "                torch.tensor(next_states, dtype=torch.float, device=device),\n",
    "                torch.tensor(rewards, dtype=torch.float, device=device),\n",
    "                torch.tensor(isgameon, dtype=torch.bool, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, Ni, Nh1, Nh2, No=4):  \n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(Ni, Nh1)\n",
    "        self.fc2 = nn.Linear(Nh1, Nh2)\n",
    "        self.fc3 = nn.Linear(Nh2, No)\n",
    "        self.act = nn.SiLU()\n",
    "        self.weights = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qloss(batch, net, gamma):\n",
    "    states, actions, next_states, rewards, _ = batch\n",
    "    lbatch = len(states)\n",
    "    state_action_values = net(states.view(lbatch, -1))\n",
    "    state_action_values = state_action_values.gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "    next_state_values = net(next_states.view(lbatch, -1)).max(1)[0].detach()\n",
    "    target = rewards + gamma * next_state_values\n",
    "    return F.smooth_l1_loss(state_action_values, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeEnvironment:    \n",
    "    def __init__(self, maze):\n",
    "        self.graph_maze = maze.to_graph()\n",
    "        self.maze = maze.to_grid()\n",
    "        self.maze_size = self.maze.size\n",
    "        x = len(self.maze)\n",
    "        y = len(self.maze)\n",
    "        self.boundary = np.asarray([x, y])\n",
    "        self.init_position = np.array([1, 1])\n",
    "        self.current_position = self.init_position.copy()\n",
    "        self.goal = (x-2, y-2)\n",
    "        self.visited = {tuple(self.current_position)}\n",
    "        self.allowed_states = np.argwhere(self.maze == 0).tolist()\n",
    "        distances = np.linalg.norm(np.array(self.allowed_states) - self.goal, axis=1)\n",
    "        goal_idx = np.where(distances == 0)[0][0]\n",
    "        self.allowed_states.pop(goal_idx)\n",
    "        self.distances = np.delete(distances, goal_idx)\n",
    "        self.action_map = {0: [0, 1], 1: [0, -1], 2: [1, 0], 3: [-1, 0]}\n",
    "        self.directions = {0: '→', 1: '←', 2: '↓', 3: '↑'}\n",
    "        self.dead_end_rewards = self.calculate_dead_end_rewards()\n",
    "\n",
    "    def reset_policy(self, eps, reg=7):\n",
    "        scaling_factor = reg * (1 - eps ** (2 / reg)) ** (reg / 2)\n",
    "        return sp.softmax(-self.distances / scaling_factor).squeeze()\n",
    "    \n",
    "    def reset(self, epsilon, prand=0):\n",
    "        if np.random.rand() < prand:\n",
    "            idx = np.random.choice(len(self.allowed_states))\n",
    "        else:\n",
    "            idx = np.random.choice(len(self.allowed_states), p=self.reset_policy(epsilon))\n",
    "        self.current_position = np.array(self.allowed_states[idx])\n",
    "        self.visited = {tuple(self.current_position)}\n",
    "        return self.state()\n",
    "    \n",
    "    def state_update(self, action):\n",
    "        isgameon = True\n",
    "        reward = -0.05\n",
    "        next_position = self.current_position + self.action_map[action]\n",
    "        if np.array_equal(self.current_position, self.goal):\n",
    "            return self.state(), -sum(self.dead_end_rewards.values()), False \n",
    "        if tuple(self.current_position) in self.visited:\n",
    "            reward = -0.2\n",
    "        if tuple(self.current_position) in self.dead_end_rewards:\n",
    "            reward += self.dead_end_rewards[tuple(self.current_position)]\n",
    "        if self.is_valid_state(next_position):\n",
    "            self.current_position = next_position\n",
    "        else:\n",
    "            reward = -1\n",
    "        self.visited.add(tuple(self.current_position))\n",
    "        return self.state(), reward, isgameon\n",
    "\n",
    "    def state(self):\n",
    "        state = copy.deepcopy(self.maze)\n",
    "        state[tuple(self.current_position)] = 2\n",
    "        return state\n",
    "        \n",
    "    def is_valid_state(self, position):\n",
    "        return not (self.is_out_of_bounds(position) or self.is_wall(position))\n",
    "\n",
    "    def is_out_of_bounds(self, position):\n",
    "        return np.any(position < 0) or np.any(position >= self.boundary)\n",
    "\n",
    "    def is_wall(self, position):\n",
    "        return self.maze[tuple(position)] == 1\n",
    "        \n",
    "    def find_dead_ends(self):\n",
    "        start_in_graph = tuple([(self.init_position[0] - 1) / 2, (self.init_position[1] - 1) / 2])\n",
    "        goal_in_graph = tuple([(self.goal[0] - 1) / 2, (self.goal[1] - 1) / 2])\n",
    "        dead_ends = {node for node, degree in self.graph_maze.degree() if degree == 1 and node not in {goal_in_graph, start_in_graph}}\n",
    "        dead_end_positions = {(2 * y + 1, 2 * x + 1) for x, y in dead_ends}\n",
    "        return dead_end_positions\n",
    "    \n",
    "    def dead_end_L1_distance(self):\n",
    "        dead_ends = self.find_dead_ends()\n",
    "        distances = {}\n",
    "        for dead_end in dead_ends:            \n",
    "            distance = abs(dead_end[0] - self.goal[0]) + abs(dead_end[1] - self.goal[1])\n",
    "            distances[dead_end] = distance  \n",
    "        return distances\n",
    "    \n",
    "    def calculate_dead_end_rewards(self):\n",
    "        dead_end_distances = self.dead_end_L1_distance()\n",
    "        max_dist = max(dead_end_distances.values())\n",
    "        min_dist = min(dead_end_distances.values())\n",
    "        if max_dist == min_dist:\n",
    "            return {dead_end: -1 for dead_end in dead_end_distances}\n",
    "        dead_end_rewards = {}\n",
    "        for dead_end, dist in dead_end_distances.items():\n",
    "            normalized_reward = -0.4 - (0.6 * (dist - min_dist) / (max_dist - min_dist))\n",
    "            dead_end_rewards[dead_end] = normalized_reward\n",
    "        return dead_end_rewards  \n",
    "\n",
    "    def draw_rewards(self):\n",
    "        rewards = np.full(self.maze.shape, np.nan)  \n",
    "        for x, y in self.allowed_states:\n",
    "            rewards[x, y] = 0\n",
    "        rewards[self.goal] = -sum(self.dead_end_rewards.values()) \n",
    "        for dead_end, reward in self.dead_end_rewards.items():\n",
    "            x, y = dead_end\n",
    "            rewards[x, y] = reward  \n",
    "        fig, ax = plt.subplots(figsize=(11, 11))\n",
    "        fig.patch.set_facecolor(\"#AACFC3\")\n",
    "        ax.set_facecolor(\"#AACFC3\")\n",
    "        cmap = plt.cm.RdYlBu_r\n",
    "        im = ax.imshow(rewards, cmap=cmap, interpolation=\"nearest\")\n",
    "        plt.colorbar(im, label=\"Reward\")\n",
    "        for x in range(self.maze.shape[0]):\n",
    "            for y in range(self.maze.shape[1]):\n",
    "                if not np.isnan(rewards[x, y]):\n",
    "                    cell_rect = patches.Rectangle(\n",
    "                        (y - 0.5, x - 0.5),\n",
    "                        1, 1,\n",
    "                        linewidth=1,\n",
    "                        edgecolor='black',\n",
    "                        facecolor='none'\n",
    "                    )\n",
    "                    ax.add_patch(cell_rect)\n",
    "                    ax.text(\n",
    "                        y, x,\n",
    "                        f\"{rewards[x, y]:.1f}\",\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        fontsize=8,\n",
    "                        color=\"black\"\n",
    "                    )\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.title(\"Reward Map\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, maze, memory_buffer, alpha=1e-6, use_softmax=True):\n",
    "        self.env = env\n",
    "        self.maze = maze\n",
    "        self.buffer = memory_buffer\n",
    "        self.alpha = alpha\n",
    "        self.num_act = 4\n",
    "        self.use_softmax = use_softmax\n",
    "        self.min_reward = -self.env.maze.size // 3\n",
    "        self.total_reward = 0\n",
    "        self.isgameon = True\n",
    "        self.rwd = 0\n",
    "\n",
    "    def make_a_move(self, net, epsilon, device='mps'):\n",
    "        current_state = self.env.state()\n",
    "        action = self.select_action(net, epsilon, device)\n",
    "        next_state, reward, self.isgameon = self.env.state_update(action)\n",
    "        self.total_reward += reward\n",
    "        self.rwd = reward\n",
    "        if self.total_reward < self.min_reward:\n",
    "            self.isgameon = False\n",
    "        if not self.isgameon:\n",
    "            self.total_reward = 0\n",
    "        self.buffer.append(Transition(current_state, action, next_state, reward, self.isgameon))\n",
    "\n",
    "    def select_action(self, net, epsilon, device='mps'):\n",
    "        state_tensor = torch.tensor(self.env.state(), dtype=torch.float32, device=device).view(1, -1)\n",
    "        qvalues = net(state_tensor).cpu().detach().numpy().squeeze()\n",
    "        if self.use_softmax:\n",
    "            p = sp.softmax(qvalues / max(epsilon, 1e-8))\n",
    "            action = np.random.choice(self.num_act, p=p)\n",
    "        else:\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randint(0, self.num_act - 1)\n",
    "            else:\n",
    "                action = np.argmax(qvalues)\n",
    "        probs = np.full(self.num_act, self.alpha)\n",
    "        probs[action] = 1 - self.alpha * (self.num_act - 1)\n",
    "        return np.random.choice(self.num_act, p=probs)\n",
    "\n",
    "    def plot_policy_map(self, net):\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            fig, ax = plt.subplots(figsize=(11, 11))\n",
    "            fig.patch.set_facecolor(\"#F6EDDD\")\n",
    "            grid_size = len(self.maze.to_grid())\n",
    "            q_values_grid = np.full((grid_size, grid_size), np.nan)\n",
    "            q_values_list = []\n",
    "            for free_cell in self.env.allowed_states:\n",
    "                self.env.current_position = np.asarray(free_cell)\n",
    "                state_tensor = torch.Tensor(self.env.state()).to('mps').view(1, -1)\n",
    "                q_values = net(state_tensor).detach().squeeze()\n",
    "                max_q = torch.abs(torch.max(q_values)).item()\n",
    "                q_values_grid[free_cell[0], free_cell[1]] = max_q\n",
    "                q_values_list.append(max_q)\n",
    "            epsilon = 1e-8\n",
    "            q_min, q_max = min(q_values_list), max(q_values_list)\n",
    "            normalized_q_grid = (q_values_grid - q_min) / (q_max - q_min + epsilon)\n",
    "            for free_cell in self.env.allowed_states:\n",
    "                self.env.current_position = np.asarray(free_cell)\n",
    "                state_tensor = torch.Tensor(self.env.state()).to('mps').view(1, -1)\n",
    "                q_values = net(state_tensor).detach().squeeze()\n",
    "                action = torch.argmax(q_values, dim=0).item()\n",
    "                policy_direction = self.env.directions[action]\n",
    "                cell_rect = patches.Rectangle((free_cell[1] - 0.5, free_cell[0] - 0.5), 1, 1,\n",
    "                                              linewidth=1, edgecolor='black', facecolor='none')\n",
    "                ax.add_patch(cell_rect)\n",
    "                ax.text(free_cell[1], free_cell[0], policy_direction, ha='center', va='center',\n",
    "                        fontsize=9, color='black', fontweight='bold')\n",
    "            goal_cell = self.env.goal\n",
    "            goal_rect = patches.Rectangle((goal_cell[1] - 0.5, goal_cell[0] - 0.5), 1, 1,\n",
    "                                          linewidth=1, edgecolor='black', facecolor='green')\n",
    "            ax.add_patch(goal_rect)\n",
    "            ax.text(goal_cell[1], goal_cell[0], 'Goal', ha='center', va='center',\n",
    "                    fontsize=7, color='white', fontweight='bold')\n",
    "            cax = ax.imshow(normalized_q_grid, cmap='RdYlBu_r', interpolation='nearest')\n",
    "            cbar = fig.colorbar(cax, ax=ax, shrink=0.75)\n",
    "            cbar.set_label('Normalized Max Q-Value', fontsize=10)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_title('Policy Map', fontsize=14, color='black')\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, net, target, optimizer, num_epochs, cutoff, batch_size, \n",
    "          buffer_start_size, gamma, exploration_rate=-1, device=\"mps\"):\n",
    "    epsilon = np.exp(-np.arange(num_epochs) / cutoff)\n",
    "    cutoff_idx = 100 * int(num_epochs / cutoff)\n",
    "    threshold = epsilon[cutoff_idx]\n",
    "    epsilon = np.minimum(epsilon, threshold)\n",
    "\n",
    "    r_list = []\n",
    "    loss_log = []\n",
    "    best_loss = float(\"inf\")\n",
    "    running_loss = 0\n",
    "    last_counters = deque(maxlen=20)\n",
    "    last_losses = deque(maxlen=20)\n",
    "    last_results = deque(maxlen=20)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        r_sum, loss, counter = 0, 0, 0\n",
    "        eps = epsilon[epoch] if exploration_rate == -1 else exploration_rate\n",
    "\n",
    "        agent.isgameon = True\n",
    "        _ = agent.env.reset(eps, prand=0.1)\n",
    "\n",
    "        while agent.isgameon:\n",
    "            agent.make_a_move(net, eps)\n",
    "            counter += 1\n",
    "            r_sum += agent.rwd\n",
    "\n",
    "            if len(agent.buffer) < buffer_start_size:\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            batch = agent.buffer.sample(batch_size, device=device)\n",
    "\n",
    "            loss_t = Qloss(batch, net, gamma=gamma)\n",
    "            loss_t.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            loss += loss_t.item()\n",
    "\n",
    "        game_result = int((agent.env.current_position == agent.env.goal).all())\n",
    "        loss_log.append(loss)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            target.load_state_dict(net.state_dict())\n",
    "\n",
    "        if epoch > num_epochs // 20:\n",
    "            running_loss = np.mean(loss_log[-50:])\n",
    "            if running_loss < best_loss:\n",
    "                best_loss = running_loss\n",
    "                torch.save(net.state_dict(), \"files/best.torch\")\n",
    "                estop = epoch\n",
    "\n",
    "        r_list.append(r_sum)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save(r_list, \"files/r_list.torch\")\n",
    "            torch.save(loss_log, \"files/loss_log.torch\")\n",
    "\n",
    "        last_counters.append(counter)\n",
    "        last_losses.append(loss)\n",
    "        last_results.append(game_result)\n",
    "\n",
    "        avg_last_counters = np.mean(last_counters) if last_counters else 0\n",
    "        avg_last_losses = np.mean(last_losses) if last_losses else 0\n",
    "        win_last_results = np.sum(last_results)\n",
    "\n",
    "        if epoch % 20 == 0 or epoch == num_epochs - 1:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            if epoch > num_epochs // 20:\n",
    "                print(f'Best loss so far: {best_loss:.5f} at epoch {estop}')\n",
    "                print('\\n')\n",
    "\n",
    "            if epoch >= 20:\n",
    "                print(f'Epoch {epoch - 20} to {epoch}')\n",
    "                progress_bar = '#' * int(100 * (epoch / num_epochs)) + ' ' * int(100 * (1 - epoch / num_epochs))\n",
    "                print(f'[{progress_bar}]')\n",
    "                print(f'Games won in last 20: {win_last_results}')\n",
    "                print(f'Avg moves in last 20: {avg_last_counters:.2f}')\n",
    "                print(f'Avg loss in last 20: {avg_last_losses:.5f}')\n",
    "                agent.plot_policy_map(net)\n",
    "            else:\n",
    "                print('Starting...')\n",
    "\n",
    "    return net, r_list, loss_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = 5, 5\n",
    "maze = RandomizedKruskalMaze(width, height)  \n",
    "\n",
    "env = MazeEnvironment(maze)\n",
    "\n",
    "env.draw_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"mps\"\n",
    "input_size = env.maze_size\n",
    "net = DeepQNetwork(input_size, input_size, input_size, 4).to(DEVICE)\n",
    "target = DeepQNetwork(input_size, input_size, input_size, 4).to(DEVICE)\n",
    "target.load_state_dict(net.state_dict())\n",
    "\n",
    "NUM_EPOCHS = 10000\n",
    "CUTOFF = NUM_EPOCHS // 3\n",
    "BUFFER_CAPACITY = NUM_EPOCHS\n",
    "BUFFER_START_SIZE = NUM_EPOCHS // 15\n",
    "BATCH_SIZE = 24\n",
    "LEARNING_RATE = 1e-4\n",
    "GAMMA = 0.9\n",
    "\n",
    "memory = ReplayMemory(BUFFER_CAPACITY)\n",
    "agent = Agent(env, maze, memory)\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "trained_net, rewards, losses = train(\n",
    "    agent, net, target, optimizer,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    cutoff=CUTOFF,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    buffer_start_size=BUFFER_START_SIZE,\n",
    "    gamma=GAMMA,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_list = torch.load(\"files/r_list.torch\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(range(len(r_list)), r_list, linewidth=2, color='#4D7C7D', label='Accumulated Reward')\n",
    "ax.set_title('Accumulated Reward Over Epochs', fontsize=18, fontweight='bold', pad=15)\n",
    "ax.set_xlabel('Epoch', fontsize=14, fontweight='medium', labelpad=10)\n",
    "ax.set_ylabel('Accumulated Reward', fontsize=14, fontweight='medium', labelpad=10)\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax.legend(fontsize=12, loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"files/net.torch\")\n",
    "net.load_state_dict(torch.load(\"files/net.torch\"))\n",
    "\n",
    "agent.plot_policy_map(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_net = copy.deepcopy(net)\n",
    "best_net.load_state_dict(torch.load(\"files/best.torch\"))\n",
    "\n",
    "agent.plot_policy_map(best_net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
