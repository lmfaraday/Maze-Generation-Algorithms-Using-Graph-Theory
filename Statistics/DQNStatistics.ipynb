{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from MazeGenerationAlgorithms.HuntAndKill import HuntAndKillMaze\n",
    "from MazeGenerationAlgorithms.DepthFirstPrim import DepthFirstPrimMaze\n",
    "from MazeGenerationAlgorithms.RandomizedKruskal import RandomizedKruskalMaze\n",
    "from MazeGenerationAlgorithms.Sidewinder import SidewinderMaze\n",
    "from MazeGenerationAlgorithms.StochasticPrim import StochasticPrimMaze\n",
    "from MazeGenerationAlgorithms.Wilson import WilsonMaze\n",
    "from MazeGenerationAlgorithms.RandomizedPrim import RandomizedPrimMaze\n",
    "from MazeGenerationAlgorithms.DFS import DFSMaze\n",
    "from MazeGenerationAlgorithms.RecursiveBacktracker import RecursiveBacktrackerMaze\n",
    "from MazeGenerationAlgorithms.AldousBroder import AldousBroderMaze\n",
    "\n",
    "import numpy as np\n",
    "import scipy.special as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Memory',\n",
    "                        field_names=['state', 'action', 'next_state', 'reward', 'is_game_on'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size, device='mps'):\n",
    "        transitions = random.sample(self.memory, batch_size)\n",
    "        states, actions, next_states, rewards, isgameon = zip(*transitions)\n",
    "        \n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        next_states = np.array(next_states)\n",
    "        rewards = np.array(rewards)\n",
    "        isgameon = np.array(isgameon)\n",
    "        \n",
    "        return (torch.tensor(states, dtype=torch.float, device=device),\n",
    "                torch.tensor(actions, dtype=torch.long, device=device),\n",
    "                torch.tensor(next_states, dtype=torch.float, device=device),\n",
    "                torch.tensor(rewards, dtype=torch.float, device=device),\n",
    "                torch.tensor(isgameon, dtype=torch.bool, device=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, Ni, Nh1, Nh2, No=4):  \n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(Ni, Nh1)\n",
    "        self.fc2 = nn.Linear(Nh1, Nh2)\n",
    "        self.fc3 = nn.Linear(Nh2, No)\n",
    "        self.act = nn.SiLU()\n",
    "        self.weights = 0  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qloss(batch, net, gamma):\n",
    "    states, actions, next_states, rewards, _ = batch\n",
    "    lbatch = len(states)\n",
    "    state_action_values = net(states.view(lbatch, -1))\n",
    "    state_action_values = state_action_values.gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "    next_state_values = net(next_states.view(lbatch, -1)).max(1)[0].detach()\n",
    "    target = rewards + gamma * next_state_values \n",
    "    return F.smooth_l1_loss(state_action_values, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeEnvironment:    \n",
    "    def __init__(self, maze):\n",
    "        self.graph_maze = maze.to_graph()\n",
    "        self.maze = maze.to_grid()\n",
    "        self.maze_size = self.maze.size\n",
    "        x = len(self.maze)\n",
    "        y = len(self.maze)\n",
    "        self.boundary = np.asarray([x, y])\n",
    "        self.init_position = np.array([1, 1])\n",
    "        self.current_position = self.init_position.copy()\n",
    "        self.goal = (x-2, y-2)\n",
    "        self.visited = {tuple(self.current_position)}\n",
    "        self.allowed_states = np.argwhere(self.maze == 0).tolist()\n",
    "        distances = np.linalg.norm(np.array(self.allowed_states) - self.goal, axis=1)\n",
    "        goal_idx = np.where(distances == 0)[0][0]\n",
    "        self.allowed_states.pop(goal_idx)\n",
    "        self.distances = np.delete(distances, goal_idx)\n",
    "        self.action_map = {0: [0, 1], 1: [0, -1], 2: [1, 0], 3: [-1, 0]}\n",
    "        self.directions = {0: '→', 1: '←', 2: '↓', 3: '↑'}\n",
    "        self.dead_end_rewards = self.calculate_dead_end_rewards()\n",
    "\n",
    "    def reset_policy(self, eps, reg=7):\n",
    "        scaling_factor = reg * (1 - eps ** (2 / reg)) ** (reg / 2)\n",
    "        return sp.softmax(-self.distances / scaling_factor).squeeze()\n",
    "\n",
    "    def reset(self, epsilon, prand=0):\n",
    "        if np.random.rand() < prand:\n",
    "            idx = np.random.choice(len(self.allowed_states))\n",
    "        else:\n",
    "            idx = np.random.choice(len(self.allowed_states), p=self.reset_policy(epsilon))\n",
    "        self.current_position = np.array(self.allowed_states[idx])\n",
    "        self.visited = {tuple(self.current_position)}\n",
    "        return self.state()\n",
    "    \n",
    "    def state_update(self, action):\n",
    "        isgameon = True\n",
    "        reward = -0.05\n",
    "        next_position = self.current_position + self.action_map[action]\n",
    "        if np.array_equal(self.current_position, self.goal):\n",
    "            return self.state(), -sum(self.dead_end_rewards.values()) if self.dead_end_rewards else 1 , False \n",
    "        if tuple(self.current_position) in self.visited:\n",
    "            reward = -0.2\n",
    "        if tuple(self.current_position) in self.dead_end_rewards:\n",
    "            reward += self.dead_end_rewards[tuple(self.current_position)]\n",
    "        if self.is_valid_state(next_position):\n",
    "            self.current_position = next_position\n",
    "        else:\n",
    "            reward = -1\n",
    "        self.visited.add(tuple(self.current_position))\n",
    "        return self.state(), reward, isgameon\n",
    "\n",
    "    def state(self):\n",
    "        state = copy.deepcopy(self.maze)\n",
    "        state[tuple(self.current_position)] = 2\n",
    "        return state\n",
    "        \n",
    "    def is_valid_state(self, position):\n",
    "        return not (self.is_out_of_bounds(position) or self.is_wall(position))\n",
    "\n",
    "    def is_out_of_bounds(self, position):\n",
    "        return np.any(position < 0) or np.any(position >= self.boundary)\n",
    "\n",
    "    def is_wall(self, position):\n",
    "        return self.maze[tuple(position)] == 1\n",
    "        \n",
    "    def find_dead_ends(self):\n",
    "        start_in_graph = tuple([(self.init_position[0] - 1) / 2, (self.init_position[1] - 1) / 2])\n",
    "        goal_in_graph = tuple([(self.goal[0] - 1) / 2, (self.goal[1] - 1) / 2])\n",
    "        dead_ends = {node for node, degree in self.graph_maze.degree() if degree == 1 and node not in {goal_in_graph, start_in_graph}}\n",
    "        dead_end_positions = {(2 * y + 1, 2 * x + 1) for x, y in dead_ends}\n",
    "        return dead_end_positions\n",
    "    \n",
    "    def dead_end_L1_distance(self):\n",
    "        dead_ends = self.find_dead_ends()\n",
    "        distances = {}\n",
    "        for dead_end in dead_ends:            \n",
    "            distance = abs(dead_end[0] - self.goal[0]) + abs(dead_end[1] - self.goal[1])\n",
    "            distances[dead_end] = distance  \n",
    "        return distances\n",
    "    \n",
    "    def calculate_dead_end_rewards(self):\n",
    "        dead_end_distances = self.dead_end_L1_distance()\n",
    "        if not dead_end_distances:\n",
    "            return {}\n",
    "        max_dist = max(dead_end_distances.values())\n",
    "        min_dist = min(dead_end_distances.values())\n",
    "        if max_dist == min_dist:\n",
    "            return {dead_end: -1 for dead_end in dead_end_distances}\n",
    "        dead_end_rewards = {}\n",
    "        for dead_end, dist in dead_end_distances.items():\n",
    "            normalized_reward = -0.4 - (0.6 * (dist - min_dist) / (max_dist - min_dist))\n",
    "            dead_end_rewards[dead_end] = normalized_reward\n",
    "        return dead_end_rewards  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, maze, memory_buffer, alpha=1e-6, use_softmax=True):\n",
    "        self.env = env\n",
    "        self.maze = maze\n",
    "        self.buffer = memory_buffer\n",
    "        self.alpha = alpha\n",
    "        self.num_act = 4\n",
    "        self.use_softmax = use_softmax\n",
    "        self.min_reward = -self.env.maze.size // 3\n",
    "        self.total_reward = 0\n",
    "        self.isgameon = True\n",
    "        self.rwd = 0\n",
    "\n",
    "    def make_a_move(self, net, epsilon, device='mps'):\n",
    "        current_state = self.env.state()\n",
    "        action = self.select_action(net, epsilon, device)\n",
    "        next_state, reward, self.isgameon = self.env.state_update(action)\n",
    "        self.total_reward += reward\n",
    "        self.rwd = reward\n",
    "        if self.total_reward < self.min_reward:\n",
    "            self.isgameon = False\n",
    "        if not self.isgameon:\n",
    "            self.total_reward = 0\n",
    "        self.buffer.append(Transition(current_state, action, next_state, reward, self.isgameon))\n",
    "\n",
    "    def select_action(self, net, epsilon, device='mps'):\n",
    "        states = np.array(self.env.state())  \n",
    "        state_tensor = torch.tensor(states, dtype=torch.float32, device=device).view(1, -1)\n",
    "        qvalues = net(state_tensor).cpu().detach().numpy().squeeze()\n",
    "        if self.use_softmax:\n",
    "            p = sp.softmax(qvalues / max(epsilon, 1e-8))\n",
    "            action = np.random.choice(self.num_act, p=p)\n",
    "        else:\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randint(0, self.num_act - 1)\n",
    "            else:\n",
    "                action = np.argmax(qvalues)\n",
    "        probs = np.full(self.num_act, self.alpha)\n",
    "        probs[action] = 1 - self.alpha * (self.num_act - 1)\n",
    "        return np.random.choice(self.num_act, p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, net, target, optimizer, num_epochs, cutoff, batch_size, \n",
    "          buffer_start_size, gamma, exploration_rate=-1, device=\"mps\"):\n",
    "    epsilon = np.exp(-np.arange(num_epochs) / cutoff)\n",
    "    cutoff_idx = 100 * int(num_epochs / cutoff)\n",
    "    threshold = epsilon[cutoff_idx]\n",
    "    epsilon = np.minimum(epsilon, threshold)\n",
    "\n",
    "    converged_epoch = None\n",
    "    last_losses = deque(maxlen=20)\n",
    "    last_rewards = deque(maxlen=20)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        r_sum, loss, counter = 0, 0, 0\n",
    "        eps = epsilon[epoch] if exploration_rate == -1 else exploration_rate\n",
    "\n",
    "        agent.isgameon = True\n",
    "        _ = agent.env.reset(eps, prand=0.1)\n",
    "\n",
    "        while agent.isgameon:\n",
    "            agent.make_a_move(net, eps)\n",
    "            counter += 1\n",
    "            r_sum += agent.rwd\n",
    "\n",
    "            if len(agent.buffer) < buffer_start_size:\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            batch = agent.buffer.sample(batch_size, device=device)\n",
    "            loss_t = Qloss(batch, net, gamma=gamma)\n",
    "            loss_t.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            loss += loss_t.item()\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            target.load_state_dict(net.state_dict())\n",
    "\n",
    "        last_losses.append(loss)\n",
    "        last_rewards.append(r_sum)\n",
    "\n",
    "        avg_last_losses = np.mean(last_losses) if last_losses else 0\n",
    "\n",
    "        if avg_last_losses <= 0.005 and epoch > buffer_start_size and all(r > 0 for r in last_rewards):\n",
    "            converged_epoch = epoch\n",
    "            break\n",
    "\n",
    "    return converged_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = 15, 15\n",
    "DEVICE = \"mps\"\n",
    "\n",
    "NUM_EPOCHS = 10000\n",
    "CUTOFF = NUM_EPOCHS // 3\n",
    "BUFFER_CAPACITY = NUM_EPOCHS\n",
    "BUFFER_START_SIZE = NUM_EPOCHS // 20\n",
    "BATCH_SIZE = 24\n",
    "LEARNING_RATE = 1e-4\n",
    "GAMMA = 0.9\n",
    "NUM_EXPERIMENTS = 10\n",
    "\n",
    "maze_types = {\n",
    "    'Sidewinder': SidewinderMaze,         \n",
    "    'Randomized Prim\\'s': RandomizedPrimMaze,  \n",
    "    'Depth First Prim\\'s': DepthFirstPrimMaze,  \n",
    "    'Stochastic Prim\\'s': StochasticPrimMaze, \n",
    "    'Randomized Kruskal\\'s': RandomizedKruskalMaze,  \n",
    "    'Wilson\\'s': WilsonMaze,                \n",
    "    'Aldous Broder': AldousBroderMaze,     \n",
    "    'Hunt and Kill': HuntAndKillMaze,      \n",
    "    'Randomized DFS': DFSMaze,             \n",
    "    'Recursive Backtracker': RecursiveBacktrackerMaze,  \n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for maze_name, MazeClass in maze_types.items():\n",
    "    print(f\"\\n=== Running experiments for {maze_name} ===\")\n",
    "    converged_epochs = []\n",
    "    deadend_counts = []  \n",
    "    maze_sizes = []  \n",
    "    avg_manhattan_distances = []\n",
    "    avg_deadend_rewards = []\n",
    "    solution_rewards = []\n",
    "\n",
    "    for i in range(NUM_EXPERIMENTS):\n",
    "        print(f\"\\n[{maze_name}] Experiment {i + 1}/{NUM_EXPERIMENTS}\")\n",
    "\n",
    "        maze = MazeClass(width, height)\n",
    "        env = MazeEnvironment(maze)\n",
    "        input_size = env.maze_size\n",
    "\n",
    "        net = DeepQNetwork(input_size, input_size, input_size, 4).to(DEVICE)\n",
    "        target = DeepQNetwork(input_size, input_size, input_size, 4).to(DEVICE)\n",
    "        target.load_state_dict(net.state_dict())\n",
    "\n",
    "        memory = ReplayMemory(BUFFER_CAPACITY)\n",
    "        agent = Agent(env, maze, memory)\n",
    "        optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        converged_epoch = train(\n",
    "            agent, net, target, optimizer,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            cutoff=CUTOFF,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            buffer_start_size=BUFFER_START_SIZE,\n",
    "            gamma=GAMMA,\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "        deadends = [n for n, d in maze.maze.degree() if d == 1 and n not in [maze.start, maze.end]]\n",
    "        deadend_counts.append(len(deadends))\n",
    "\n",
    "        manhattan_distances = [\n",
    "            abs(dead_end[0] - maze.end[0]) + abs(dead_end[1] - maze.end[1]) for dead_end in deadends\n",
    "        ]\n",
    "        avg_manhattan_distance = np.mean(manhattan_distances) if manhattan_distances else 0\n",
    "        avg_manhattan_distances.append(avg_manhattan_distance)\n",
    "\n",
    "        max_dist = max(manhattan_distances) if manhattan_distances else 0\n",
    "        min_dist = min(manhattan_distances) if manhattan_distances else 0\n",
    "\n",
    "        if max_dist == min_dist:\n",
    "            deadend_rewards = {dead_end: -1 for dead_end in deadends}\n",
    "        else:\n",
    "            deadend_rewards = {\n",
    "                dead_end: -0.4 - (0.6 * (dist - min_dist) / (max_dist - min_dist))\n",
    "                for dead_end, dist in zip(deadends, manhattan_distances)\n",
    "            }\n",
    "\n",
    "        avg_deadend_reward = np.mean(list(deadend_rewards.values())) if deadend_rewards else 0\n",
    "        avg_deadend_rewards.append(avg_deadend_reward)\n",
    "        \n",
    "        solution_reward = -np.sum(list(deadend_rewards.values())) if deadend_rewards else 1\n",
    "        solution_rewards.append(solution_reward)\n",
    "\n",
    "        maze_size = f\"{maze.width}x{maze.height}\"\n",
    "        maze_sizes.append(maze_size)\n",
    "\n",
    "        converged_epochs.append(converged_epoch)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    for i in range(NUM_EXPERIMENTS):\n",
    "        results.append({\n",
    "            'Maze Type': maze_name,\n",
    "            'Size': maze_sizes[i],\n",
    "            '# of Deadends': deadend_counts[i],\n",
    "            'Avg Manhattan Distance from Deadends to Solution': f\"{avg_manhattan_distances[i]:.3f}\",\n",
    "            'Avg Deadend Reward': f\"{avg_deadend_rewards[i]:.3f}\",\n",
    "            'Solution Reward': f\"{solution_rewards[i]:.3f}\",\n",
    "            'Converged Epoch': converged_epochs[i]\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"DQN_experiment_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
